import pickle
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import re
import streamlit as st
import unicodedata
import string
from pyvi import ViTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
from deep_translator import GoogleTranslator

from textblob import TextBlob

import nltk

nltk.data.path.append("nltk_data")
from nltk.corpus import stopwords

from langdetect import detect
from underthesea import sentiment, pos_tag
from wordcloud import WordCloud


# Load mÃ´ hÃ¬nh vÃ  dá»¯ liá»‡u
df_info = pd.read_pickle("df_info_Recommendation.pkl")
tfidf = pd.read_pickle("tfidf_vectorizer_Recommendation.pkl")


def recommend_hotels_by_description_sklearn(
    query_text, top_n=5, alpha=0.5, beta=0.2, gamma=0.15, delta=0.15
):
    query_vec = tfidf.transform([query_text])
    tfidf_matrix = tfidf.transform(df_info["Clean_Description"])
    sim_scores = cosine_similarity(query_vec, tfidf_matrix)[0]
    df_info["Similarity_Score"] = sim_scores
    df_info["Final_Score"] = (
        alpha * df_info["Similarity_Score"]
        + beta * df_info["Rank_Score"]
        + gamma * df_info["Comment_Score"]
        + delta * df_info["Total_Score_Score"]
    )
    top_hotels = df_info.sort_values(by="Final_Score", ascending=False).head(top_n)
    return top_hotels[
        [
            "Hotel_Name",
            "Hotel_Rank",
            "Total_Score",
            "comments_count",
            "Clean_Description",
            "Final_Score",
        ]
    ]


def extract_date(text):
    if not isinstance(text, str):
        return None
    match = re.search(r"(\d{1,2}) thÃ¡ng (\d{1,2}) (\d{4})", text)
    if match:
        day, month, year = match.groups()
        return f"{day}/{month}/{year}"
    return None


# Load cÃ¡c tá»« Ä‘iá»ƒn há»— trá»£
def load_dict(path):
    with open(path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    return dict(line.strip().split("\t") for line in lines if "\t" in line)


# nltk.download("punkt", quiet=True)
# nltk.download("stopwords", quiet=True)

teencode_dict = load_dict("teencode.txt")
emoji_dict = load_dict("emojicon.txt")
english_vn_dict = load_dict("english-vnmese.txt")
stopwords_data = set(
    open("vietnamese-stopwords.txt", encoding="utf-8").read().splitlines()
)

# Táº¡o bá»™ dá»‹ch kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  sá»‘
translator = str.maketrans(
    string.punctuation + string.digits,
    " " * (len(string.punctuation) + len(string.digits)),
)

# Táº¡o regex tá»•ng há»£p cho emoji, teencode, vÃ  english-vn
emoji_pattern = re.compile("|".join(map(re.escape, emoji_dict.keys())))
teen_pattern = re.compile(
    r"\b(" + "|".join(map(re.escape, teencode_dict.keys())) + r")\b"
)
eng_pattern = re.compile(
    r"\b(" + "|".join(map(re.escape, english_vn_dict.keys())) + r")\b"
)


# HÃ m chuáº©n hÃ³a vÄƒn báº£n
def normalize_text_old(text):
    if not isinstance(text, str):
        return ""

    # Chuáº©n hÃ³a unicode
    text = unicodedata.normalize("NFKC", text)

    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  sá»‘
    text = text.translate(translator)

    # Thay tháº¿ emoji
    text = emoji_pattern.sub(lambda m: f" {emoji_dict[m.group()]} ", text)

    # Thay tháº¿ teencode
    text = teen_pattern.sub(lambda m: teencode_dict[m.group()], text)

    # Dá»‹ch tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t
    text = eng_pattern.sub(lambda m: english_vn_dict[m.group()], text)

    # âœ… Tokenize báº±ng PyVi
    text = ViTokenizer.tokenize(text)

    # Chuyá»ƒn vá» chá»¯ thÆ°á»ng
    text = text.lower()

    # Loáº¡i bá» stopwords
    words = text.split()
    words = [w for w in words if w not in stopwords_data and len(w) > 1]

    return " ".join(words)


def show_hotel_info(hotel_id):
    hotel = df_info[df_info["Hotel_ID"].astype(str) == hotel_id].iloc[0]
    st.write(f"\nðŸ“Œ KhÃ¡ch sáº¡n: {hotel['Hotel_Name']} (ID: {hotel_id})")
    st.write(f"ðŸ… Háº¡ng: {hotel['Hotel_Rank']}")
    st.write(f"ðŸ“ Äá»‹a chá»‰: {hotel['Hotel_Address']}")
    st.write(f"â­ Tá»•ng Ä‘iá»ƒm: {hotel['Total_Score']}")
    st.write(f"ðŸ“ MÃ´ táº£: {hotel['Hotel_Description'][:300]}...")


def analyze_strengths_and_weaknesses(hotel_id):
    # Láº¥y thÃ´ng tin khÃ¡ch sáº¡n
    hotel = df_info[df_info["Hotel_ID"].astype(str) == hotel_id].iloc[0]
    hotel_comments = df_comments[df_comments["Hotel ID"].astype(str) == hotel_id]

    # CÃ¡c cá»™t Ä‘iá»ƒm chi tiáº¿t
    score_cols = [
        "Location",
        "Cleanliness",
        "Service",
        "Facilities",
        "Value_for_money",
        "Comfort_and_room_quality",
    ]

    # TÃ­nh trung bÃ¬nh há»‡ thá»‘ng
    system_avg = df_info[score_cols].mean()

    print("\nðŸ“Š PhÃ¢n tÃ­ch Ä‘iá»ƒm máº¡nh & Ä‘iá»ƒm yáº¿u:")

    for col in score_cols:
        hotel_score = hotel[col]
        avg_score = system_avg[col]

        if pd.notnull(hotel_score):
            diff = hotel_score - avg_score
            status = "âœ… Äiá»ƒm máº¡nh" if diff > 0 else "âš ï¸ Äiá»ƒm yáº¿u"
            st.write(
                f"- {col.replace('_', ' ').title()}: {hotel_score:.2f} ({status}, trung bÃ¬nh há»‡ thá»‘ng: {avg_score:.2f})"
            )
        else:
            st.write(f"- {col.replace('_', ' ').title()}: KhÃ´ng cÃ³ dá»¯ liá»‡u")

    # PhÃ¢n tÃ­ch sá»‘ lÆ°á»£ng nháº­n xÃ©t
    num_reviews = len(hotel_comments)
    st.write(f"\nðŸ§® Sá»‘ lÆ°á»£ng nháº­n xÃ©t: {num_reviews} lÆ°á»£t")

    # PhÃ¢n tÃ­ch ná»™i dung nháº­n xÃ©t (tá»« khÃ³a ná»•i báº­t)
    hotel_comments["Normalized_Text"] = hotel_comments["Review_Text"].apply(
        normalize_text_old
    )
    hotel_comments = hotel_comments[hotel_comments["Normalized_Text"].str.strip() != ""]

    vectorizer = TfidfVectorizer(max_features=15)
    X = vectorizer.fit_transform(hotel_comments["Normalized_Text"])
    keywords = vectorizer.get_feature_names_out()

    st.write("\nðŸ—£ï¸ Tá»« khÃ³a ná»•i báº­t trong nháº­n xÃ©t:")
    st.write(", ".join(keywords))


def analyze_customers(hotel_id):
    # Lá»c dá»¯ liá»‡u theo Hotel ID
    hotel_comments = df_comments[df_comments["Hotel ID"].astype(str) == str(hotel_id)]

    if hotel_comments.empty:
        st.write(f"âŒ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cho Hotel ID: {hotel_id}")
        return

    # 1ï¸âƒ£ Quá»‘c tá»‹ch phá»• biáº¿n
    top_nationalities = hotel_comments["Nationality"].value_counts().head(5)
    st.write("\nðŸŒ Quá»‘c tá»‹ch phá»• biáº¿n:")
    for nat, count in top_nationalities.items():
        print(f"- {nat}: {count} lÆ°á»£t")

    # Váº½ biá»ƒu Ä‘á»“ trÃ²n quá»‘c tá»‹ch
    plt.figure(figsize=(6, 6))
    plt.pie(
        top_nationalities.values,
        labels=top_nationalities.index,
        autopct="%1.1f%%",
        startangle=140,
    )
    plt.title(f"ðŸŒ Quá»‘c tá»‹ch phá»• biáº¿n - Hotel ID: {hotel_id}")
    plt.axis("equal")
    plt.tight_layout()
    st.pyplot(plt)

    # 2ï¸âƒ£ NhÃ³m khÃ¡ch phá»• biáº¿n
    if "Group Name" in hotel_comments.columns:
        top_groups = hotel_comments["Group Name"].value_counts().head(5)
        st.write("\nðŸ‘¥ NhÃ³m khÃ¡ch phá»• biáº¿n:")
        for grp, count in top_groups.items():
            st.write(f"- {grp}: {count} lÆ°á»£t")

        # Váº½ biá»ƒu Ä‘á»“ trÃ²n nhÃ³m khÃ¡ch
        plt.figure(figsize=(6, 6))
        plt.pie(
            top_groups.values,
            labels=top_groups.index,
            autopct="%1.1f%%",
            startangle=140,
        )
        plt.title(f"ðŸ‘¥ NhÃ³m khÃ¡ch phá»• biáº¿n - Hotel ID: {hotel_id}")
        plt.axis("equal")
        plt.tight_layout()
        st.pyplot(plt)
    else:
        st.write("\nâš ï¸ KhÃ´ng cÃ³ cá»™t 'Group Name' Ä‘á»ƒ phÃ¢n loáº¡i nhÃ³m khÃ¡ch.")

    # 3ï¸âƒ£ Xu hÆ°á»›ng theo thá»i gian + Váº½ biá»ƒu Ä‘á»“
    if "Review_Month" in hotel_comments.columns:
        monthly_trend = hotel_comments.groupby("Review_Month").size()

        if monthly_trend.empty:
            st.write("\nâš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“.")
            return

        st.write("\nðŸ“ˆ Xu hÆ°á»›ng Ä‘Ã¡nh giÃ¡ theo thá»i gian:")

        # Chuyá»ƒn Period vá» Timestamp Ä‘á»ƒ váº½
        monthly_trend.index = monthly_trend.index.to_timestamp()

        # Váº½ biá»ƒu Ä‘á»“ Ä‘Æ°á»ng
        plt.figure(figsize=(12, 6))
        sns.lineplot(
            x=monthly_trend.index, y=monthly_trend.values, marker="o", color="darkcyan"
        )
        plt.title(
            f"ðŸ“ˆ Xu hÆ°á»›ng Ä‘Ã¡nh giÃ¡ theo thá»i gian - Hotel ID: {hotel_id}", fontsize=14
        )
        plt.xlabel("ThÃ¡ng", fontsize=12)
        plt.ylabel("Sá»‘ lÆ°á»£t Ä‘Ã¡nh giÃ¡", fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(True)
        plt.tight_layout()
        st.pyplot(plt)
    else:
        st.write("\nâš ï¸ KhÃ´ng cÃ³ cá»™t 'Review_Month' Ä‘á»ƒ phÃ¢n tÃ­ch thá»i gian.")


def normalize_text(text, lang):
    try:
        if not isinstance(text, str):
            return ""

        # Náº¿u lÃ  tiáº¿ng Anh, dá»‹ch sang tiáº¿ng Viá»‡t trÆ°á»›c
        if lang == "en":
            # Loáº¡i bá» stopwords tiáº¿ng Anh
            stopwords_en = set(stopwords.words("english"))
            words = [
                word
                for word in nltk.word_tokenize(text)
                if word.lower() not in stopwords_en and len(word) > 2
            ]
            filtered_text = " ".join(words).lower()

            # Dá»‹ch tá»«ng tá»« sang tiáº¿ng Viá»‡t náº¿u cÃ³ trong tá»« Ä‘iá»ƒn
            translated_text = eng_pattern.sub(
                lambda m: english_vn_dict.get(m.group(), m.group()), filtered_text
            )

            # GÃ¡n láº¡i text vÃ  chuyá»ƒn sang xá»­ lÃ½ nhÆ° tiáº¿ng Viá»‡t
            text = translated_text
            lang = "vi"  # chuyá»ƒn sang pipeline tiáº¿ng Viá»‡t

        if lang == "vi":
            # Chuáº©n hÃ³a vÃ  thay tháº¿
            text = unicodedata.normalize("NFKC", text)
            text = text.translate(translator)
            text = emoji_pattern.sub(lambda m: f" {emoji_dict[m.group()]} ", text)
            text = teen_pattern.sub(lambda m: teencode_dict[m.group()], text)
            text = eng_pattern.sub(lambda m: english_vn_dict[m.group()], text)

            # TÃ¡ch tá»« vÃ  lá»c stopwords
            text = ViTokenizer.tokenize(text).lower()
            words = [w for w in text.split() if w not in stopwords_data and len(w) > 1]
            cleaned_text = " ".join(words)

            # POS tagging
            tagged = pos_tag(cleaned_text)

            selected = []
            phrases = []

            for i in range(len(tagged) - 1):
                word1, tag1 = tagged[i]
                word2, tag2 = tagged[i + 1]

                if tag1 == "R" and tag2 == "A":
                    phrases.append(f"{word1.lower()}_{word2.lower()}")

            for word, tag in tagged:
                if tag in ["A", "R"]:
                    selected.append(word.lower())

            # Loáº¡i tá»« Ä‘Æ¡n náº¿u Ä‘Ã£ náº±m trong cá»¥m
            flattened = set()
            for phrase in phrases:
                flattened.update(phrase.split("_"))
            selected = [word for word in selected if word not in flattened]

            return " ".join(phrases + selected)

        else:
            return text

    except Exception as e:
        st.write(f"âš ï¸ Lá»—i normalize_text: {e}")
        return text


def classify_keywords(keywords: str, lang: str):
    results = {}
    for kw in keywords.split():
        try:
            clean_kw = kw.replace("_", " ")  # âœ… bá» dáº¥u gáº¡ch dÆ°á»›i Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘Ãºng
            valid_labels = ["positive", "negative"]
            
            if lang == "vi":
                # Dá»‹ch sang tiáº¿ng Anh
                translated_kw = GoogleTranslator(source='vi', target='en').translate(clean_kw)
                polarity = TextBlob(translated_kw).sentiment.polarity
                sentiment_label = (
                    "positive"
                    if polarity > 0.1
                    else "negative" if polarity < -0.1 else "neutral"
                )
                results[kw] = sentiment_label

            elif lang == "en":
                polarity = TextBlob(clean_kw).sentiment.polarity
                sentiment_label = (
                    "positive"
                    if polarity > 0.1
                    else "negative" if polarity < -0.1 else "neutral"
                )
                results[kw] = sentiment_label

            else:
                results[kw] = "neutral"

        except:
            results[kw] = "neutral"
    return results


def analyze_keywords(hotel_id):
    # Lá»c nháº­n xÃ©t theo Hotel ID
    hotel_comments = df_comments[df_comments["Hotel ID"].astype(str) == str(hotel_id)]

    if hotel_comments.empty:
        st.write(f"âŒ KhÃ´ng tÃ¬m tháº¥y nháº­n xÃ©t cho Hotel ID: {hotel_id}")
        return

    # Loáº¡i nháº­n xÃ©t trá»‘ng
    hotel_comments = hotel_comments[
        hotel_comments["Review_Text"].notnull()
        & (hotel_comments["Review_Text"].str.strip() != "")
    ]

    # Nháº­n diá»‡n ngÃ´n ngá»¯
    def detect_language(text):
        try:
            return detect(text)
        except:
            return "unknown"

    hotel_comments["Lang"] = hotel_comments["Review_Text"].apply(detect_language)

    # Chuáº©n hÃ³a vÄƒn báº£n
    hotel_comments["Processed_Text"] = hotel_comments.apply(
        lambda row: normalize_text(row["Review_Text"], row["Lang"]), axis=1
    )

    # PhÃ¢n tÃ­ch cáº£m xÃºc tá»«ng nháº­n xÃ©t
    def classify_sentiment(row):
        text = row["Processed_Text"]
        lang = row["Lang"]
        try:
            if lang == "vi":
                translated = GoogleTranslator(source="vi", target="en").translate(text)
                polarity = TextBlob(translated).sentiment.polarity
                return (
                    "positive"
                    if polarity > 0.1
                    else "negative" if polarity < -0.1 else "neutral"
                )
            elif lang == "en":
                polarity = TextBlob(text).sentiment.polarity
                return (
                    "positive"
                    if polarity > 0.1
                    else "negative" if polarity < -0.1 else "neutral"
                )
            else:
                return "neutral"
        except:
            return "neutral"

    hotel_comments["Sentiment"] = hotel_comments.apply(classify_sentiment, axis=1)
    
    # Gom táº¥t cáº£ tá»« khÃ³a Ä‘Ã£ chuáº©n hÃ³a
    all_keywords = []
    for _, row in hotel_comments.iterrows():
        kw_dict = classify_keywords(row["Processed_Text"], row["Lang"])
        for kw, senti in kw_dict.items():
            if senti in ["positive", "negative"]:
                all_keywords.append((kw, senti))
    
    # Gom tá»« khÃ³a theo cáº£m xÃºc
    pos_keywords = set([kw for kw, senti in all_keywords if senti == "positive"])
    neg_keywords = set([kw for kw, senti in all_keywords if senti == "negative"])

    # Loáº¡i bá» giao nhau
    shared = pos_keywords & neg_keywords
    pos_keywords -= shared
    neg_keywords -= shared

    # In káº¿t quáº£
    st.write("\nâœ… Tá»« khÃ³a tÃ­ch cá»±c:")
    st.write(
        ", ".join(sorted(pos_keywords))
        if pos_keywords
        else "KhÃ´ng cÃ³ tá»« tÃ­ch cá»±c ná»•i báº­t."
    )

    st.write("\nâš ï¸ Tá»« khÃ³a tiÃªu cá»±c:")
    st.write(
        ", ".join(sorted(neg_keywords))
        if neg_keywords
        else "KhÃ´ng cÃ³ tá»« tiÃªu cá»±c ná»•i báº­t."
    )

    # Váº½ Word Cloud
    def show_wordcloud(keywords, title, color="black"):
        if not keywords:
            print(f"âš ï¸ KhÃ´ng cÃ³ tá»« khÃ³a Ä‘á»ƒ váº½ Word Cloud cho: {title}")
            return
        text = " ".join(keywords)
        wc = WordCloud(
            width=800, height=400, background_color="white", colormap=color
        ).generate(text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        plt.title(title, fontsize=16)
        plt.tight_layout()
        st.pyplot(plt)

    show_wordcloud(pos_keywords, "âœ… Word Cloud - Tá»« khÃ³a tÃ­ch cá»±c", color="Greens")
    show_wordcloud(neg_keywords, "âš ï¸ Word Cloud - Tá»« khÃ³a tiÃªu cá»±c", color="Reds")


def compare_to_system(hotel_id):
    hotel = df_info[df_info["Hotel_ID"].astype(str) == hotel_id].iloc[0]
    system_avg = df_info[score_cols].mean()

    st.write("\nðŸ“Š So sÃ¡nh Ä‘iá»ƒm tá»«ng tiÃªu chÃ­ vá»›i trung bÃ¬nh há»‡ thá»‘ng:")
    for col in score_cols:
        if pd.notnull(hotel[col]):
            diff = hotel[col] - system_avg[col]
            st.write(
                f"- {col.replace('_', ' ').title()}: {diff:+.2f} Ä‘iá»ƒm so vá»›i trung bÃ¬nh"
            )


st.set_page_config(page_title="Hotel Recommender", layout="wide")

# Sidebar menu
menu = st.sidebar.selectbox(
    "ðŸ“‚ Chá»n má»¥c",
    [
        "Business Problem",
        "Evaluation & Report",
        "Recommendation",
        "Hotel Insight by Hotel ID",
        "ThÃ´ng tin nhÃ³m",
    ],
)

if menu == "Business Problem":
    st.header("ðŸŽ¯ Business Problem")
    st.markdown(
        """
    #### ðŸ¨ Vá» Agoda
    - Agoda lÃ  má»™t trang web Ä‘áº·t phÃ²ng trá»±c tuyáº¿n cÃ³ trá»¥ sá»Ÿ táº¡i Singapore, Ä‘Æ°á»£c thÃ nh láº­p vÃ o nÄƒm 2005, thuá»™c sá»Ÿ há»¯u cá»§a **Booking Holdings Inc.**
    - Agoda chuyÃªn cung cáº¥p dá»‹ch vá»¥ Ä‘áº·t phÃ²ng khÃ¡ch sáº¡n, cÄƒn há»™, nhÃ  nghá»‰ vÃ  cÃ¡c loáº¡i hÃ¬nh lÆ°u trÃº trÃªn toÃ n cáº§u.
    - Trang web nÃ y cho phÃ©p ngÆ°á»i dÃ¹ng tÃ¬m kiáº¿m, so sÃ¡nh vÃ  Ä‘áº·t chá»— á»Ÿ vá»›i má»©c giÃ¡ Æ°u Ä‘Ã£i.

    #### ðŸ¤– BÃ i toÃ¡n Ä‘áº·t ra
    - Giáº£ sá»­ Agoda **chÆ°a triá»ƒn khai há»‡ thá»‘ng Recommender System** giÃºp Ä‘á» xuáº¥t khÃ¡ch sáº¡n/resort phÃ¹ há»£p tá»›i ngÆ°á»i dÃ¹ng.
    - YÃªu cáº§u xÃ¢y dá»±ng há»‡ thá»‘ng gá»£i Ã½ thÃ´ng minh dá»±a trÃªn thÃ´ng tin mÃ´ táº£, Ä‘Ã¡nh giÃ¡, vá»‹ trÃ­, vÃ  hÃ nh vi ngÆ°á»i dÃ¹ng.

    #### ðŸ“Š GÃ³c nhÃ¬n tá»« phÃ­a chá»§ khÃ¡ch sáº¡n
    - Chá»§ khÃ¡ch sáº¡n muá»‘n náº¯m rÃµ **insight tá»« khÃ¡ch hÃ ng**: há» thÃ­ch gÃ¬, Ä‘Ã¡nh giÃ¡ ra sao, mÃ´ táº£ nÃ o thu hÃºt nháº¥t.
    - App cung cáº¥p cho há» cÃ¡c phÃ¢n tÃ­ch, báº£ng tá»•ng há»£p, vÃ  cÃ´ng cá»¥ giÃºp tá»‘i Æ°u hÃ³a tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng.

    #### ðŸŽ“ Bá»‘i cáº£nh Ä‘á»“ Ã¡n
    - ÄÃ¢y lÃ  má»™t pháº§n trong **Äá»“ Ã¡n tá»‘t nghiá»‡p ngÃ nh Data Science**, nÆ¡i báº¡n váº­n dá»¥ng NLP, Machine Learning vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n thá»±c táº¿.
    """
    )

    st.markdown(
        """
    #### ðŸŽ¯ Má»¥c tiÃªu giáº£i phÃ¡p
    - XÃ¢y dá»±ng há»‡ thá»‘ng Ä‘á» xuáº¥t thÃ´ng minh nháº±m há»— trá»£ ngÆ°á»i dÃ¹ng **nhanh chÃ³ng chá»n Ä‘Æ°á»£c nÆ¡i lÆ°u trÃº phÃ¹ há»£p** trÃªn ná»n táº£ng Agoda.
    - TÄƒng tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng, giáº£m thá»i gian tÃ¬m kiáº¿m, vÃ  nÃ¢ng cao tá»· lá»‡ chuyá»ƒn Ä‘á»•i Ä‘áº·t phÃ²ng.

    #### ðŸ§© Kiáº¿n trÃºc há»‡ thá»‘ng gá»£i Ã½
    Há»‡ thá»‘ng sáº½ bao gá»“m **hai mÃ´ hÃ¬nh gá»£i Ã½ chÃ­nh**:

    - ðŸ” **Content-based Filtering**  
      Dá»±a trÃªn ná»™i dung mÃ´ táº£, Ä‘áº·c Ä‘iá»ƒm cá»§a khÃ¡ch sáº¡n (vá»‹ trÃ­, tiá»‡n nghi, Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡...) Ä‘á»ƒ gá»£i Ã½ cÃ¡c khÃ¡ch sáº¡n tÆ°Æ¡ng tá»± vá»›i nhu cáº§u ngÆ°á»i dÃ¹ng.

    - ðŸ‘¥ **Collaborative Filtering**  
      Dá»±a trÃªn hÃ nh vi vÃ  Ä‘Ã¡nh giÃ¡ cá»§a ngÆ°á»i dÃ¹ng khÃ¡c cÃ³ sá»Ÿ thÃ­ch tÆ°Æ¡ng Ä‘á»“ng Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½ cÃ¡ nhÃ¢n hÃ³a.

    #### ðŸ“Š Cung cáº¥p insight cho chá»§ khÃ¡ch sáº¡n
    - PhÃ¢n tÃ­ch thÃ´ng tin khÃ¡ch hÃ ng dÃ nh cho tá»«ng khÃ¡ch sáº¡n Ä‘á»ƒ giÃºp chá»§ khÃ¡ch sáº¡n hiá»ƒu rÃµ:
        - âœ… **Äiá»ƒm máº¡nh**: nhá»¯ng yáº¿u tá»‘ Ä‘Æ°á»£c khÃ¡ch hÃ ng Ä‘Ã¡nh giÃ¡ cao
        - âš ï¸ **Äiá»ƒm cáº§n cáº£i thiá»‡n**: nhá»¯ng váº¥n Ä‘á» thÆ°á»ng bá»‹ pháº£n Ã¡nh hoáº·c cÃ³ Ä‘iá»ƒm sá»‘ tháº¥p
    - Tá»« Ä‘Ã³, chá»§ khÃ¡ch sáº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh dá»‹ch vá»¥, mÃ´ táº£, hoáº·c chiáº¿n lÆ°á»£c giÃ¡ Ä‘á»ƒ thu hÃºt thÃªm khÃ¡ch hÃ ng tiá»m nÄƒng.

    #### ðŸ›  CÃ´ng nghá»‡ sá»­ dá»¥ng
    - Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) Ä‘á»ƒ phÃ¢n tÃ­ch mÃ´ táº£ vÃ  Ä‘Ã¡nh giÃ¡
    - TF-IDF, cosine similarity, matrix factorization
    - Trá»±c quan hÃ³a dá»¯ liá»‡u báº±ng Streamlit
    """
    )

elif menu == "Evaluation & Report":
    st.header("ðŸ“Š Evaluation & Report")

    tab1, tab2, tab3, tab4 = st.tabs(
        [
            "ðŸ” Content-based Filtering: Cosine Similarity",
            "ðŸ“š Content-based Filtering: Gensim",
            "ðŸ‘¥ Collaborative Filtering: ALS",
            "âš–ï¸ So sÃ¡nh model",
        ]
    )

    with tab1:
        st.subheader(
            "ðŸ” Äá» xuáº¥t ngÆ°á»i dÃ¹ng vá»›i Content-based Filtering â€“ PhÆ°Æ¡ng phÃ¡p Cosine Similarity"
        )
        st.markdown(
            """
        - Ã tÆ°á»Ÿng chÃ­nh cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  Ä‘Æ°a ra gá»£i Ã½ dá»±a vÃ o **sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c sáº£n pháº©m**.
        - Má»—i mÃ´ táº£ khÃ¡ch sáº¡n Ä‘Æ°á»£c vector hÃ³a báº±ng TF-IDF.
        - Sau Ä‘Ã³, tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a truy váº¥n ngÆ°á»i dÃ¹ng vÃ  cÃ¡c mÃ´ táº£ báº±ng **cosine similarity**.
        - Nhá»¯ng khÃ¡ch sáº¡n cÃ³ Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t sáº½ Ä‘Æ°á»£c gá»£i Ã½.
        """
        )
        st.image(
            "cosine_similarity.jpg",
            caption="Minh há»a phÆ°Æ¡ng phÃ¡p Cosine Similarity",
            use_column_width=True,
        )

    with tab2:
        st.subheader(
            "ðŸ“š Äá» xuáº¥t ngÆ°á»i dÃ¹ng vá»›i Content-based Filtering â€“ PhÆ°Æ¡ng phÃ¡p Gensim"
        )
        st.markdown(
            """
        - **Gensim** lÃ  má»™t thÆ° viá»‡n Python chuyÃªn xÃ¡c Ä‘á»‹nh sá»± tÆ°Æ¡ng tá»± vá» ngá»¯ nghÄ©a giá»¯a hai tÃ i liá»‡u thÃ´ng qua mÃ´ hÃ¬nh khÃ´ng gian vector vÃ  bá»™ cÃ´ng cá»¥ mÃ´ hÃ¬nh hÃ³a chá»§ Ä‘á».
        - CÃ³ thá»ƒ xá»­ lÃ½ kho dá»¯ liá»‡u vÄƒn báº£n lá»›n vá»›i sá»± trá»£ giÃºp cá»§a viá»‡c truyá»n dá»¯ liá»‡u hiá»‡u quáº£ vÃ  cÃ¡c thuáº­t toÃ¡n tÄƒng cÆ°á»ng.
        - Tá»‘c Ä‘á»™ xá»­ lÃ½ vÃ  tá»‘i Æ°u hÃ³a viá»‡c sá»­ dá»¥ng bá»™ nhá»› tá»‘t.
        - Tuy nhiÃªn, Gensim cÃ³ **Ã­t tÃ¹y chá»n tÃ¹y biáº¿n** cho cÃ¡c function so vá»›i scikit-learn.
        """
        )
        st.image(
            "gensim.jpg", caption="Minh há»a phÆ°Æ¡ng phÃ¡p Gensim", use_column_width=True
        )

    with tab3:
        st.subheader("ðŸ‘¥ Collaborative Filtering vá»›i ALS (Alternating Least Squares)")
        st.markdown(
            """
        - **ALS** lÃ  má»™t ká»¹ thuáº­t máº¡nh máº½ trong há»‡ thá»‘ng gá»£i Ã½, Ä‘áº·c biá»‡t hiá»‡u quáº£ khi xá»­ lÃ½ **dá»¯ liá»‡u lá»›n vÃ  thÆ°a (sparse data)**.
        - ÄÃ¢y lÃ  má»™t dáº¡ng **Matrix Factorization** giÃºp dá»± Ä‘oÃ¡n má»©c Ä‘á»™ yÃªu thÃ­ch cá»§a ngÆ°á»i dÃ¹ng Ä‘á»‘i vá»›i sáº£n pháº©m mÃ  há» chÆ°a tá»«ng tÆ°Æ¡ng tÃ¡c.
        - ALS chia ma tráº­n tÆ°Æ¡ng tÃ¡c thÃ nh hai ma tráº­n nhá» hÆ¡n: má»™t Ä‘áº¡i diá»‡n cho ngÆ°á»i dÃ¹ng, má»™t cho sáº£n pháº©m.
        - Báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a luÃ¢n phiÃªn (alternating), mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng tiá»m áº©n vÃ  Ä‘Æ°a ra gá»£i Ã½ chÃ­nh xÃ¡c.

        #### ðŸ“‰ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
        - RMSE (Root Mean Square Error) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh.
        - RMSE cÃ ng tháº¥p â†’ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cÃ ng sÃ¡t vá»›i thá»±c táº¿.
        """
        )
        st.image(
            "rmse_als.jpg",
            caption="Biá»ƒu Ä‘á»“ RMSE cá»§a mÃ´ hÃ¬nh ALS",
            use_column_width=True,
        )
        st.image(
            "ALS_minhhoa.jpg", caption="Minh há»a kiáº¿n trÃºc ALS", use_column_width=True
        )

    with tab4:
        st.subheader("âš–ï¸ So sÃ¡nh:")
        st.markdown(
            """
        #### ðŸ“ˆ Nháº­n xÃ©t tá»•ng quan
        - Äiá»ƒm RMSE cá»§a mÃ´ hÃ¬nh ALS lÃ  0.6, hÆ¡i cao nÃªn khÃ´ng tá»‘t
        - CÃ³ thá»ƒ tháº¥y **TF-IDF thÆ°á»ng cho Ä‘iá»ƒm cao hÆ¡n Gensim** khi tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng.
        - Äiá»u nÃ y xuáº¥t phÃ¡t tá»« báº£n cháº¥t cá»§a hai phÆ°Æ¡ng phÃ¡p:

        **1. TF-IDF thiÃªn vá» táº§n suáº¥t tá»«**
        - TF-IDF chá»‰ dá»±a vÃ o táº§n suáº¥t vÃ  Ä‘á»™ hiáº¿m cá»§a tá»« trong vÄƒn báº£n.
        - Náº¿u hai vÄƒn báº£n cÃ³ nhiá»u tá»« giá»‘ng nhau â†’ Ä‘iá»ƒm similarity sáº½ cao.
        - Trong táº­p dá»¯ liá»‡u, cÃ³ thá»ƒ nhiá»u mÃ´ táº£ cÃ³ tá»« giá»‘ng nhau â†’ TF-IDF dá»… cho Ä‘iá»ƒm cao.

        **2. Gensim hiá»ƒu ngá»¯ nghÄ©a**
        - Gensim (Doc2Vec hoáº·c Word2Vec) dÃ¹ng embedding Ä‘á»ƒ biá»ƒu diá»…n vÄƒn báº£n.
        - Hiá»ƒu Ä‘Æ°á»£c ngá»¯ cáº£nh, cáº¥u trÃºc cÃ¢u, vÃ  má»‘i quan há»‡ giá»¯a tá»«.
        - VÃ¬ embedding lÃ  khÃ´ng gian liÃªn tá»¥c â†’ Ä‘iá»ƒm cosine similarity thÆ°á»ng nhá» hÆ¡n.
        - Gensim cho Ä‘iá»ƒm tháº¥p hÆ¡n, nhÆ°ng **phÃ¢n biá»‡t tá»‘t hÆ¡n** giá»¯a cÃ¡c vÄƒn báº£n thá»±c sá»± giá»‘ng nhau vá» Ã½ nghÄ©a.
        """
        )
        st.image(
            "compare_cosine_gensim.jpg",
            caption="So sÃ¡nh Cosine Similarity vÃ  Gensim",
            use_column_width=True,
        )

        st.markdown(
            """
    #### âœ… Káº¿t luáº­n
    - Vá»›i Ä‘áº·c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  káº¿t quáº£ Ä‘Ã¡nh giÃ¡, mÃ´ hÃ¬nh **Content-based Filtering** sá»­ dá»¥ng **Cosine Similarity** lÃ  lá»±a chá»n phÃ¹ há»£p.
    - PhÆ°Æ¡ng phÃ¡p nÃ y táº­n dá»¥ng tá»‘t thÃ´ng tin mÃ´ táº£ sáº£n pháº©m vÃ  kháº£ nÄƒng phÃ¢n biá»‡t ngá»¯ nghÄ©a cá»§a Gensim.
    - DÃ¹ Ä‘iá»ƒm similarity tháº¥p hÆ¡n TF-IDF, nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c trong viá»‡c gá»£i Ã½ sáº£n pháº©m phÃ¹ há»£p láº¡i cao hÆ¡n.
    - Do Ä‘Ã³, nÃªn Æ°u tiÃªn sá»­ dá»¥ng **Cosine Similarity vá»›i embedding tá»« Gensim** Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng gá»£i Ã½.
    """
        )


elif menu == "Recommendation":
    st.header("ðŸ’¡ Recommendation")
    st.write("Gá»£i Ã½ khÃ¡ch sáº¡n theo mÃ´ táº£, Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡, vá»‹ trÃ­...")

    query = st.text_input("Nháº­p mÃ´ táº£ khÃ¡ch sáº¡n báº¡n muá»‘n tÃ¬m:")
    if query:
        results = recommend_hotels_by_description_sklearn(query)
        st.write(results)

elif menu == "Hotel Insight by Hotel ID":
    st.header("ðŸ” Hotel Insight by Hotel ID")

    # Load dá»¯ liá»‡u khÃ¡ch sáº¡n
    df_info = pd.read_csv("hotel_info.csv")

    # Äá»c dá»¯ liá»‡u
    df_comments = pd.read_csv("hotel_comments.csv")

    # Chuáº©n hÃ³a df_info
    score_cols = [
        "Location",
        "Cleanliness",
        "Service",
        "Facilities",
        "Value_for_money",
        "Comfort_and_room_quality",
    ]

    for col in score_cols:
        df_info[col] = pd.to_numeric(
            df_info["Total_Score"].astype(str).str.replace(",", "."), errors="coerce"
        )

    # Chuáº©n hÃ³a df_comments
    df_comments["Score"] = pd.to_numeric(
        df_comments["Score"].astype(str).str.replace(",", "."), errors="coerce"
    )
    df_comments["Review_Date_Clean"] = df_comments["Review Date"].apply(extract_date)
    df_comments["Review_Date_Clean"] = pd.to_datetime(
        df_comments["Review_Date_Clean"], format="%d/%m/%Y", errors="coerce"
    )
    df_comments["Title"] = df_comments["Title"].fillna("")
    df_comments["Body"] = df_comments["Body"].fillna("")
    df_comments["Review_Text"] = df_comments["Title"] + " " + df_comments["Body"]
    df_comments["Review_Month"] = df_comments["Review_Date_Clean"].dt.to_period("M")

    hotel_id = st.text_input("Nháº­p Hotel_ID cáº§n phÃ¢n tÃ­ch: ")

    if hotel_id:
        if hotel_id.strip() == "":
            st.info("ðŸ”Ž Vui lÃ²ng nháº­p Hotel_ID Ä‘á»ƒ báº¯t Ä‘áº§u phÃ¢n tÃ­ch.")
        elif hotel_id not in df_info["Hotel_ID"].astype(str).values:
            st.error("âŒ Hotel_ID khÃ´ng tá»“n táº¡i.")
        else:
            show_hotel_info(hotel_id)
            analyze_strengths_and_weaknesses(hotel_id)
            analyze_customers(hotel_id)
            analyze_keywords(hotel_id)
            compare_to_system(hotel_id)
    else:
        st.info("ðŸ”Ž Nháº­p Hotel_ID Ä‘á»ƒ báº¯t Ä‘áº§u phÃ¢n tÃ­ch.")

elif menu == "ThÃ´ng tin nhÃ³m":
    st.header("ðŸ‘¥ NhÃ³m E thá»±c hiá»‡n")
    st.markdown(
        """
    **Há» tÃªn HV 1**: Nguyá»…n Trung HÆ°ng  
    **Há» tÃªn HV 2**: Nguyá»…n VÅ© Báº£o TrÃ¢n  
    """
    )






